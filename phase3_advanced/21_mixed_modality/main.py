"""
æ¨¡å— 21ï¼šæ··åˆæ¨¡æ€
å­¦ä¹ å¦‚ä½•å¤„ç†æ–‡æœ¬ã€å›¾åƒç­‰å¤šç§æ¨¡æ€çš„è¾“å…¥

âš ï¸ é‡è¦æç¤ºï¼š
1. æœ¬æ¨¡å—éœ€è¦æ”¯æŒè§†è§‰çš„æ¨¡å‹ï¼ˆå¦‚ OpenAI çš„ gpt-4o-miniï¼‰
2. DeepSeek ç›®å‰ä¸æ”¯æŒå›¾åƒè¾“å…¥ï¼Œè¯·æ›´æ¢ä¸º OpenAI æ¨¡å‹
3. è¯·åœ¨ images/ ç›®å½•ä¸‹æ”¾ç½®ä½ è‡ªå·±çš„æµ‹è¯•å›¾ç‰‡

ä½¿ç”¨å‰å‡†å¤‡ï¼š
1. åœ¨ .env ä¸­è®¾ç½® OPENAI_API_KEY
2. åœ¨ images/ ç›®å½•ä¸‹æ”¾ç½®æµ‹è¯•å›¾ç‰‡
"""

import os
import base64
from pathlib import Path
from typing import TypedDict, List, Optional
from dotenv import load_dotenv

from langgraph.graph import StateGraph, START, END
from langchain.chat_models import init_chat_model
from langchain_core.messages import HumanMessage, SystemMessage

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

if not OPENAI_API_KEY or OPENAI_API_KEY == "your_openai_api_key_here":
    raise ValueError(
        "\nè¯·å…ˆåœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®æœ‰æ•ˆçš„ OPENAI_API_KEY\n"
        "å›¾åƒå¤„ç†éœ€è¦ä½¿ç”¨ OpenAI çš„è§†è§‰æ¨¡å‹\n"
        "è®¿é—® https://platform.openai.com/ è·å–å¯†é’¥"
    )

# åˆå§‹åŒ–æ¨¡å‹ï¼ˆå›¾åƒå¤„ç†éœ€è¦æ”¯æŒè§†è§‰çš„æ¨¡å‹ï¼‰
model = init_chat_model("openai:gpt-4o-mini", api_key=OPENAI_API_KEY)

# å›¾ç‰‡ç›®å½•
IMAGES_DIR = Path(__file__).parent / "images"

# ============================================================
# è¾…åŠ©å‡½æ•°
# ============================================================

def encode_image_to_base64(image_path: str) -> str:
    """å°†æœ¬åœ°å›¾åƒç¼–ç ä¸º base64"""
    with open(image_path, "rb") as image_file:
        return base64.standard_b64encode(image_file.read()).decode("utf-8")

def get_mime_type(image_path: str) -> str:
    """æ ¹æ®æ–‡ä»¶æ‰©å±•åè·å– MIME ç±»å‹"""
    ext = Path(image_path).suffix.lower()
    mime_types = {
        ".jpg": "image/jpeg",
        ".jpeg": "image/jpeg",
        ".png": "image/png",
        ".gif": "image/gif",
        ".webp": "image/webp"
    }
    return mime_types.get(ext, "image/jpeg")

def create_image_content(image_path: str) -> dict:
    """åˆ›å»ºå›¾åƒå†…å®¹å—"""
    if not os.path.exists(image_path):
        raise FileNotFoundError(f"å›¾ç‰‡ä¸å­˜åœ¨: {image_path}")
    
    image_base64 = encode_image_to_base64(image_path)
    mime_type = get_mime_type(image_path)
    
    return {
        "type": "image_url",
        "image_url": {"url": f"data:{mime_type};base64,{image_base64}"}
    }

def check_image_exists(filename: str) -> str:
    """æ£€æŸ¥å›¾ç‰‡æ˜¯å¦å­˜åœ¨"""
    image_path = IMAGES_DIR / filename
    if not image_path.exists():
        print(f"âš ï¸ å›¾ç‰‡ä¸å­˜åœ¨: {image_path}")
        return None
    return str(image_path)

# ============================================================
# ç¤ºä¾‹ 1ï¼šæ–‡æœ¬ + å›¾åƒæ··åˆè¾“å…¥
# ============================================================

def example_1_text_and_image():
    """
    å¤„ç†æ–‡æœ¬å’Œå›¾åƒçš„æ··åˆè¾“å…¥
    """
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹ 1ï¼šæ–‡æœ¬ + å›¾åƒæ··åˆè¾“å…¥")
    print("=" * 60)
    
    image_path = check_image_exists("chart.png")
    if not image_path:
        print("è¯·åœ¨ images/ ç›®å½•ä¸‹æ”¾ç½® chart.pngï¼ˆå›¾è¡¨å›¾ç‰‡ï¼‰")
        print("è·³è¿‡æ­¤ç¤ºä¾‹")
        return None
    
    # åˆ›å»ºæ··åˆå†…å®¹æ¶ˆæ¯
    content = [
        {"type": "text", "text": """ä»¥ä¸‹æ˜¯æˆ‘ä»¬å…¬å¸çš„é”€å”®æ•°æ®ï¼š

**2024å¹´ç¬¬ä¸€å­£åº¦é”€å”®æŠ¥å‘Š**
- 1æœˆ: 150ä¸‡
- 2æœˆ: 180ä¸‡  
- 3æœˆ: 220ä¸‡

è¯·ç»“åˆå›¾è¡¨åˆ†æï¼š
1. æ•°æ®è¶‹åŠ¿å¦‚ä½•ï¼Ÿ
2. ä¸å›¾è¡¨æ˜¾ç¤ºçš„è¶‹åŠ¿æ˜¯å¦ä¸€è‡´ï¼Ÿ
3. ä½ æœ‰ä»€ä¹ˆå»ºè®®ï¼Ÿ"""},
        create_image_content(image_path)
    ]
    
    message = HumanMessage(content=content)
    
    print("ğŸ“Š å‘é€æ–‡æœ¬æ•°æ® + å›¾è¡¨å›¾ç‰‡...")
    
    response = model.invoke([message])
    
    print("\nğŸ¤– åˆ†æç»“æœï¼š")
    print(response.content)
    
    return response.content

# ============================================================
# ç¤ºä¾‹ 2ï¼šå¤šå›¾åƒå¯¹æ¯”åˆ†æ
# ============================================================

def example_2_multi_image():
    """
    å¯¹æ¯”å¤šå¼ å›¾ç‰‡
    """
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹ 2ï¼šå¤šå›¾åƒå¯¹æ¯”åˆ†æ")
    print("=" * 60)
    
    # æ£€æŸ¥å›¾ç‰‡
    image1_path = check_image_exists("image1.jpg")
    image2_path = check_image_exists("image2.jpg")
    
    if not image1_path or not image2_path:
        print("è¯·åœ¨ images/ ç›®å½•ä¸‹æ”¾ç½® image1.jpg å’Œ image2.jpg")
        print("è·³è¿‡æ­¤ç¤ºä¾‹")
        return None
    
    content = [
        {"type": "text", "text": "è¯·å¯¹æ¯”è¿™ä¸¤å¼ å›¾ç‰‡ï¼Œè¯´æ˜å®ƒä»¬çš„ç›¸åŒç‚¹å’Œä¸åŒç‚¹ã€‚"},
        create_image_content(image1_path),
        create_image_content(image2_path)
    ]
    
    message = HumanMessage(content=content)
    
    print("ğŸ“· å¯¹æ¯”ä¸¤å¼ å›¾ç‰‡...")
    
    response = model.invoke([message])
    
    print("\nğŸ” å¯¹æ¯”ç»“æœï¼š")
    print(response.content)
    
    return response.content

# ============================================================
# ç¤ºä¾‹ 3ï¼šä½¿ç”¨ LangGraph å¤„ç†æ··åˆæ¨¡æ€
# ============================================================

class MultimodalState(TypedDict):
    """æ··åˆæ¨¡æ€çŠ¶æ€"""
    text_input: str
    image_paths: List[str]
    analysis_result: Optional[str]
    summary: Optional[str]

def example_3_langgraph_multimodal():
    """
    ä½¿ç”¨ LangGraph æ„å»ºæ··åˆæ¨¡æ€å¤„ç†æµç¨‹
    """
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹ 3ï¼šLangGraph æ··åˆæ¨¡æ€å¤„ç†")
    print("=" * 60)
    
    # æ£€æŸ¥å›¾ç‰‡
    image_path = check_image_exists("sample.jpg")
    if not image_path:
        print("è¯·åœ¨ images/ ç›®å½•ä¸‹æ”¾ç½® sample.jpg")
        print("è·³è¿‡æ­¤ç¤ºä¾‹")
        return None
    
    # å®šä¹‰èŠ‚ç‚¹å‡½æ•°
    def analyze_content(state: MultimodalState) -> MultimodalState:
        """åˆ†ææ··åˆå†…å®¹"""
        print("ğŸ“ æ­£åœ¨åˆ†æå†…å®¹...")
        
        content = [{"type": "text", "text": state["text_input"]}]
        
        for img_path in state["image_paths"]:
            if os.path.exists(img_path):
                content.append(create_image_content(img_path))
        
        message = HumanMessage(content=content)
        response = model.invoke([message])
        
        state["analysis_result"] = response.content
        return state
    
    def summarize(state: MultimodalState) -> MultimodalState:
        """æ€»ç»“åˆ†æç»“æœ"""
        print("ğŸ“‹ æ­£åœ¨ç”Ÿæˆæ€»ç»“...")
        
        message = HumanMessage(
            content=f"è¯·ç”¨3å¥è¯æ€»ç»“ä»¥ä¸‹åˆ†æï¼š\n\n{state['analysis_result']}"
        )
        response = model.invoke([message])
        
        state["summary"] = response.content
        return state
    
    # æ„å»ºå›¾
    graph = StateGraph(MultimodalState)
    
    graph.add_node("analyze", analyze_content)
    graph.add_node("summarize", summarize)
    
    graph.add_edge(START, "analyze")
    graph.add_edge("analyze", "summarize")
    graph.add_edge("summarize", END)
    
    workflow = graph.compile()
    
    # è¿è¡Œ
    initial_state = {
        "text_input": "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡ï¼ŒåŒ…æ‹¬ä¸»è¦å†…å®¹ã€è‰²å½©å’Œæ°›å›´ã€‚",
        "image_paths": [image_path],
        "analysis_result": None,
        "summary": None
    }
    
    result = workflow.invoke(initial_state)
    
    print("\nğŸ“Š è¯¦ç»†åˆ†æï¼š")
    print(result["analysis_result"])
    print("\nğŸ“Œ æ€»ç»“ï¼š")
    print(result["summary"])
    
    return result

# ============================================================
# ç¤ºä¾‹ 4ï¼šäº¤äº’å¼å›¾åƒé—®ç­”
# ============================================================

def example_4_interactive_qa():
    """
    åŸºäºå›¾åƒçš„äº¤äº’å¼é—®ç­”
    """
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹ 4ï¼šäº¤äº’å¼å›¾åƒé—®ç­”ï¼ˆæ¼”ç¤ºï¼‰")
    print("=" * 60)
    
    image_path = check_image_exists("sample.jpg")
    if not image_path:
        print("è¯·åœ¨ images/ ç›®å½•ä¸‹æ”¾ç½® sample.jpg")
        print("è·³è¿‡æ­¤ç¤ºä¾‹")
        return None
    
    # æ¨¡æ‹Ÿé—®ç­”æµç¨‹
    questions = [
        "è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†ä»€ä¹ˆï¼Ÿ",
        "å›¾ç‰‡ä¸­æœ‰å“ªäº›é¢œè‰²ï¼Ÿ",
        "ä½ è§‰å¾—è¿™å¼ å›¾ç‰‡æ˜¯åœ¨ä»€ä¹ˆåœºæ™¯ä¸‹æ‹æ‘„çš„ï¼Ÿ"
    ]
    
    messages = []
    
    # é¦–å…ˆå‘é€å›¾ç‰‡
    initial_content = [
        {"type": "text", "text": "æˆ‘å°†åŸºäºè¿™å¼ å›¾ç‰‡é—®ä½ ä¸€äº›é—®é¢˜ã€‚"},
        create_image_content(image_path)
    ]
    messages.append(HumanMessage(content=initial_content))
    
    print(f"ğŸ“· å·²åŠ è½½å›¾ç‰‡: {image_path}\n")
    
    for q in questions:
        print(f"â“ é—®é¢˜: {q}")
        
        messages.append(HumanMessage(content=q))
        response = model.invoke(messages)
        messages.append(response)
        
        print(f"ğŸ’¬ å›ç­”: {response.content}\n")
    
    print("ğŸ’¡ æç¤º: å®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨ input() å®ç°çœŸæ­£çš„äº¤äº’å¼é—®ç­”")
    
    return messages

# ============================================================
# ä¸»ç¨‹åº
# ============================================================

if __name__ == "__main__":
    print("=" * 60)
    print("æ··åˆæ¨¡æ€å¤„ç†æ•™ç¨‹")
    print("=" * 60)
    
    print("""
âš ï¸ ä½¿ç”¨å‰è¯·ç¡®ä¿ï¼š
1. å·²åœ¨ .env ä¸­è®¾ç½® OPENAI_API_KEY
2. å·²åœ¨ images/ ç›®å½•ä¸‹æ”¾ç½®æµ‹è¯•å›¾ç‰‡:
   - sample.jpg: é€šç”¨æµ‹è¯•å›¾ç‰‡
   - chart.png: å›¾è¡¨å›¾ç‰‡
   - image1.jpg, image2.jpg: ç”¨äºå¯¹æ¯”çš„å›¾ç‰‡

å¦‚æœæ²¡æœ‰å‡†å¤‡å›¾ç‰‡ï¼Œç›¸å…³ç¤ºä¾‹å°†è¢«è·³è¿‡ã€‚
""")
    
    # åˆ›å»ºå›¾ç‰‡ç›®å½•
    IMAGES_DIR.mkdir(exist_ok=True)
    
    # è¿è¡Œç¤ºä¾‹
    example_1_text_and_image()
    example_2_multi_image()
    example_3_langgraph_multimodal()
    example_4_interactive_qa()
    
    print("\n" + "=" * 60)
    print("âœ… æ•™ç¨‹è¿è¡Œå®Œæˆï¼")
    print("=" * 60)
    print("""
ğŸ’¡ å­¦ä¹ è¦ç‚¹ï¼š
1. æ··åˆæ¨¡æ€æ¶ˆæ¯çš„æ„å»ºæ–¹å¼
2. å¤šå›¾åƒè¾“å…¥çš„å¤„ç†
3. ä½¿ç”¨ LangGraph å¤„ç†æ··åˆæ¨¡æ€æµç¨‹
4. äº¤äº’å¼å›¾åƒé—®ç­”çš„å®ç°

ğŸ“ ä¸‹ä¸€æ­¥ï¼š
- å°è¯•ä½¿ç”¨è‡ªå·±çš„å›¾ç‰‡
- ä¿®æ”¹æç¤ºè¯è§‚å¯Ÿæ•ˆæœå˜åŒ–
- ç»“åˆ RAG å®ç°æ›´å¤æ‚çš„åº”ç”¨
""")
