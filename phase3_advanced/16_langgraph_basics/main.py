"""
æ¨¡å— 16ï¼šLangGraph åŸºç¡€
å­¦ä¹ å¦‚ä½•ä½¿ç”¨ LangGraph åˆ›å»ºçŠ¶æ€å›¾å·¥ä½œæµ
"""

import os
from typing import TypedDict, Annotated, Literal
from dotenv import load_dotenv

from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver
from langchain.chat_models import init_chat_model
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()
GROQ_API_KEY = os.getenv("GROQ_API_KEY")

if not GROQ_API_KEY or GROQ_API_KEY == "your_groq_api_key_here":
    raise ValueError(
        "\nè¯·å…ˆåœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®æœ‰æ•ˆçš„ GROQ_API_KEY\n"
        "è®¿é—® https://console.groq.com/keys è·å–å…è´¹å¯†é’¥"
    )

# åˆå§‹åŒ–æ¨¡å‹
model = init_chat_model("groq:llama-3.3-70b-versatile", api_key=GROQ_API_KEY)

# ============================================================
# ç¤ºä¾‹ 1ï¼šç®€å•é¡ºåºå·¥ä½œæµ
# ============================================================

def simple_workflow():
    """
    æœ€ç®€å•çš„ LangGraph ç¤ºä¾‹ï¼šé¡ºåºæ‰§è¡Œçš„èŠ‚ç‚¹
    æµç¨‹ï¼šSTART -> é¢„å¤„ç† -> LLMå¤„ç† -> åå¤„ç† -> END
    """
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹ 1ï¼šç®€å•é¡ºåºå·¥ä½œæµ")
    print("=" * 60)

    # å®šä¹‰çŠ¶æ€
    class SimpleState(TypedDict):
        input_text: str
        processed_text: str
        llm_response: str
        final_output: str

    # åˆå§‹åŒ–æ¨¡å‹
    
    # å®šä¹‰èŠ‚ç‚¹å‡½æ•°
    def preprocess(state: SimpleState) -> dict:
        """é¢„å¤„ç†èŠ‚ç‚¹ï¼šæ¸…ç†å’Œæ ¼å¼åŒ–è¾“å…¥"""
        text = state["input_text"].strip().lower()
        print(f"  [é¢„å¤„ç†] è¾“å…¥: '{state['input_text']}' -> '{text}'")
        return {"processed_text": text}

    def call_llm(state: SimpleState) -> dict:
        """LLM èŠ‚ç‚¹ï¼šè°ƒç”¨è¯­è¨€æ¨¡å‹"""
        messages = [
            SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„åŠ©æ‰‹ï¼Œè¯·ç®€æ´å›ç­”é—®é¢˜ã€‚"),
            HumanMessage(content=state["processed_text"])
        ]
        response = model.invoke(messages)
        print(f"  [LLM] å“åº”: {response.content[:50]}...")
        return {"llm_response": response.content}

    def postprocess(state: SimpleState) -> dict:
        """åå¤„ç†èŠ‚ç‚¹ï¼šæ ¼å¼åŒ–è¾“å‡º"""
        final = f"âœ¨ AI å›å¤ï¼š{state['llm_response']}"
        print("  [åå¤„ç†] å®Œæˆæ ¼å¼åŒ–")
        return {"final_output": final}

    # æ„å»ºå›¾
    graph = StateGraph(SimpleState)
    
    # æ·»åŠ èŠ‚ç‚¹
    graph.add_node("preprocess", preprocess)
    graph.add_node("call_llm", call_llm)
    graph.add_node("postprocess", postprocess)
    
    # æ·»åŠ è¾¹ï¼ˆå®šä¹‰æ‰§è¡Œé¡ºåºï¼‰
    graph.add_edge(START, "preprocess")
    graph.add_edge("preprocess", "call_llm")
    graph.add_edge("call_llm", "postprocess")
    graph.add_edge("postprocess", END)
    
    # ç¼–è¯‘å›¾
    app = graph.compile()
    
    # å¯è§†åŒ–å›¾ç»“æ„ï¼ˆæ‰“å°ï¼‰
    print("\nå›¾ç»“æ„ï¼šSTART -> preprocess -> call_llm -> postprocess -> END")
    
    # è¿è¡Œ
    result = app.invoke({"input_text": "  ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ  "})
    
    print(f"\næœ€ç»ˆè¾“å‡ºï¼š\n{result['final_output']}")
    
    return result

# ============================================================
# ç¤ºä¾‹ 2ï¼šæ¡ä»¶åˆ†æ”¯å·¥ä½œæµ
# ============================================================

def conditional_workflow():
    """
    å¸¦æ¡ä»¶åˆ†æ”¯çš„å·¥ä½œæµ
    æ ¹æ®è¾“å…¥å†…å®¹é€‰æ‹©ä¸åŒçš„å¤„ç†è·¯å¾„
    """
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹ 2ï¼šæ¡ä»¶åˆ†æ”¯å·¥ä½œæµ")
    print("=" * 60)

    class ConditionalState(TypedDict):
        query: str
        query_type: str
        response: str

    
    def classify_query(state: ConditionalState) -> dict:
        """åˆ†ç±»èŠ‚ç‚¹ï¼šåˆ¤æ–­æŸ¥è¯¢ç±»å‹"""
        query = state["query"].lower()
        
        if any(word in query for word in ["å¤©æ°”", "æ¸©åº¦", "ä¸‹é›¨"]):
            query_type = "weather"
        elif any(word in query for word in ["è®¡ç®—", "åŠ ", "å‡", "ä¹˜", "é™¤", "ç­‰äº"]):
            query_type = "math"
        else:
            query_type = "general"
        
        print(f"  [åˆ†ç±»] æŸ¥è¯¢ç±»å‹: {query_type}")
        return {"query_type": query_type}

    def handle_weather(state: ConditionalState) -> dict:
        """å¤„ç†å¤©æ°”æŸ¥è¯¢"""
        print("  [å¤©æ°”å¤„ç†] æ‰§è¡Œå¤©æ°”æŸ¥è¯¢é€»è¾‘...")
        # å®é™…åº”ç”¨ä¸­è¿™é‡Œä¼šè°ƒç”¨å¤©æ°” API
        response = "ğŸŒ¤ï¸ ä»Šå¤©å¤©æ°”æ™´æœ—ï¼Œæ¸©åº¦ 25Â°Cï¼Œé€‚åˆå¤–å‡ºï¼"
        return {"response": response}

    def handle_math(state: ConditionalState) -> dict:
        """å¤„ç†æ•°å­¦è®¡ç®—"""
        print("  [æ•°å­¦å¤„ç†] æ‰§è¡Œè®¡ç®—é€»è¾‘...")
        messages = [
            SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªæ•°å­¦åŠ©æ‰‹ï¼Œè¯·è®¡ç®—å¹¶ç»™å‡ºç»“æœã€‚"),
            HumanMessage(content=state["query"])
        ]
        result = model.invoke(messages)
        return {"response": f"ğŸ”¢ {result.content}"}

    def handle_general(state: ConditionalState) -> dict:
        """å¤„ç†ä¸€èˆ¬æŸ¥è¯¢"""
        print("  [é€šç”¨å¤„ç†] æ‰§è¡Œé€šç”¨ LLM è°ƒç”¨...")
        messages = [
            SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†æ¸Šåšçš„åŠ©æ‰‹ï¼Œè¯·å›ç­”é—®é¢˜ã€‚"),
            HumanMessage(content=state["query"])
        ]
        result = model.invoke(messages)
        return {"response": f"ğŸ’¡ {result.content}"}

    def route_query(state: ConditionalState) -> Literal["weather", "math", "general"]:
        """è·¯ç”±å‡½æ•°ï¼šè¿”å›ä¸‹ä¸€ä¸ªèŠ‚ç‚¹åç§°"""
        return state["query_type"]

    # æ„å»ºå›¾
    graph = StateGraph(ConditionalState)
    
    # æ·»åŠ èŠ‚ç‚¹
    graph.add_node("classify", classify_query)
    graph.add_node("weather", handle_weather)
    graph.add_node("math", handle_math)
    graph.add_node("general", handle_general)
    
    # æ·»åŠ è¾¹
    graph.add_edge(START, "classify")
    
    # æ·»åŠ æ¡ä»¶è¾¹
    graph.add_conditional_edges(
        "classify",  # ä»å“ªä¸ªèŠ‚ç‚¹å‡ºå‘
        route_query,  # è·¯ç”±å‡½æ•°
        {  # è·¯ç”±æ˜ å°„
            "weather": "weather",
            "math": "math",
            "general": "general"
        }
    )
    
    # æ‰€æœ‰å¤„ç†èŠ‚ç‚¹éƒ½åˆ° END
    graph.add_edge("weather", END)
    graph.add_edge("math", END)
    graph.add_edge("general", END)
    
    # ç¼–è¯‘
    app = graph.compile()
    
    # æµ‹è¯•ä¸åŒç±»å‹çš„æŸ¥è¯¢
    test_queries = [
        "ä»Šå¤©åŒ—äº¬çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
        "è®¡ç®— 123 åŠ  456 ç­‰äºå¤šå°‘ï¼Ÿ",
        "Python æ˜¯ä»€ä¹ˆç¼–ç¨‹è¯­è¨€ï¼Ÿ"
    ]
    
    for query in test_queries:
        print(f"\næŸ¥è¯¢: {query}")
        result = app.invoke({"query": query})
        print(f"å“åº”: {result['response'][:100]}...")
    
    return result

# ============================================================
# ç¤ºä¾‹ 3ï¼šå¸¦å†…å­˜çš„å¯¹è¯å·¥ä½œæµ
# ============================================================

def conversation_workflow():
    """
    å¸¦å†…å­˜çš„å¤šè½®å¯¹è¯å·¥ä½œæµ
    ä½¿ç”¨ add_messages æ³¨è§£è‡ªåŠ¨ç®¡ç†æ¶ˆæ¯å†å²
    """
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹ 3ï¼šå¸¦å†…å­˜çš„å¯¹è¯å·¥ä½œæµ")
    print("=" * 60)

    # ä½¿ç”¨ add_messages æ³¨è§£ï¼Œæ¶ˆæ¯ä¼šè‡ªåŠ¨è¿½åŠ 
    class ConversationState(TypedDict):
        messages: Annotated[list, add_messages]
        turn_count: int

    
    def chat_node(state: ConversationState) -> dict:
        """å¯¹è¯èŠ‚ç‚¹ï¼šå¤„ç†ç”¨æˆ·æ¶ˆæ¯å¹¶ç”Ÿæˆå›å¤"""
        # æ·»åŠ ç³»ç»Ÿæç¤ºï¼ˆå¦‚æœæ˜¯ç¬¬ä¸€è½®ï¼‰
        messages = state["messages"]
        if not any(isinstance(m, SystemMessage) for m in messages):
            messages = [
                SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„ä¸­æ–‡åŠ©æ‰‹ã€‚è®°ä½ç”¨æˆ·å‘Šè¯‰ä½ çš„ä¿¡æ¯ã€‚")
            ] + messages
        
        # è°ƒç”¨æ¨¡å‹
        response = model.invoke(messages)
        
        # æ›´æ–°è½®æ•°
        turn_count = state.get("turn_count", 0) + 1
        print(f"  [å¯¹è¯è½®æ¬¡ {turn_count}] AI: {response.content[:50]}...")
        
        # è¿”å›æ–°æ¶ˆæ¯ï¼ˆä¼šè‡ªåŠ¨è¿½åŠ åˆ° messages åˆ—è¡¨ï¼‰
        return {
            "messages": [response],  # add_messages ä¼šè‡ªåŠ¨è¿½åŠ 
            "turn_count": turn_count
        }

    def should_continue(state: ConversationState) -> Literal["continue", "end"]:
        """å†³å®šæ˜¯å¦ç»§ç»­å¯¹è¯"""
        # è¿™é‡Œç®€åŒ–ä¸ºæ€»æ˜¯è¿”å› endï¼Œå®é™…åº”ç”¨ä¸­å¯ä»¥æ£€æŸ¥ç”¨æˆ·æ„å›¾
        return "end"

    # æ„å»ºå›¾
    graph = StateGraph(ConversationState)
    
    graph.add_node("chat", chat_node)
    
    graph.add_edge(START, "chat")
    graph.add_edge("chat", END)  # ç®€åŒ–ï¼šæ¯æ¬¡è°ƒç”¨å¤„ç†ä¸€è½®
    
    # ä½¿ç”¨å†…å­˜ä¿å­˜å™¨
    memory = MemorySaver()
    app = graph.compile(checkpointer=memory)
    
    # æ¨¡æ‹Ÿå¤šè½®å¯¹è¯ï¼ˆä½¿ç”¨ç›¸åŒçš„ thread_idï¼‰
    config = {"configurable": {"thread_id": "user_001"}}
    
    conversations = [
        "ä½ å¥½ï¼æˆ‘å«å°æ˜ã€‚",
        "æˆ‘æœ€å–œæ¬¢çš„ç¼–ç¨‹è¯­è¨€æ˜¯ Pythonã€‚",
        "ä½ è¿˜è®°å¾—æˆ‘çš„åå­—å—ï¼Ÿ",
        "æˆ‘å–œæ¬¢ä»€ä¹ˆç¼–ç¨‹è¯­è¨€ï¼Ÿ"
    ]
    
    for user_input in conversations:
        print(f"\nç”¨æˆ·: {user_input}")
        result = app.invoke(
            {"messages": [HumanMessage(content=user_input)]},
            config=config
        )
        print(f"AI: {result['messages'][-1].content}")
    
    # æŸ¥çœ‹å®Œæ•´å¯¹è¯å†å²
    print("\n" + "-" * 40)
    print("å®Œæ•´å¯¹è¯å†å²ï¼š")
    state = app.get_state(config)
    for msg in state.values["messages"]:
        role = "ç”¨æˆ·" if isinstance(msg, HumanMessage) else "AI"
        print(f"  [{role}] {msg.content[:60]}...")
    
    return result

# ============================================================
# ä¸»ç¨‹åº
# ============================================================

if __name__ == "__main__":
    print("LangGraph åŸºç¡€æ•™ç¨‹")
    print("=" * 60)
    
    # è¿è¡Œç¤ºä¾‹
    simple_workflow()
    conditional_workflow()
    conversation_workflow()
    
    print("\n" + "=" * 60)
    print("âœ… æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆï¼")
    print("=" * 60)
