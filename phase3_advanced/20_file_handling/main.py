"""
æ¨¡å— 20ï¼šæ–‡ä»¶å¤„ç†
å­¦ä¹ å¦‚ä½•åŠ è½½å’Œå¤„ç†å„ç§æ–‡ä»¶ç±»å‹
"""

import os
import tempfile
from typing import List
from dotenv import load_dotenv

from langchain.chat_models import init_chat_model
from langchain_core.documents import Document
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_text_splitters import RecursiveCharacterTextSplitter

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()
GROQ_API_KEY = os.getenv("GROQ_API_KEY")

if not GROQ_API_KEY or GROQ_API_KEY == "your_groq_api_key_here":
    raise ValueError(
        "\nè¯·å…ˆåœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®æœ‰æ•ˆçš„ GROQ_API_KEY\n"
        "è®¿é—® https://console.groq.com/keys è·å–å…è´¹å¯†é’¥"
    )

# åˆå§‹åŒ–æ¨¡å‹
model = init_chat_model("groq:llama-3.3-70b-versatile", api_key=GROQ_API_KEY)

    # åˆå§‹åŒ–æ¨¡å‹

# ============================================================
# è¾…åŠ©å‡½æ•°ï¼šåˆ›å»ºç¤ºä¾‹æ–‡ä»¶
# ============================================================

def create_sample_files():
    """åˆ›å»ºç”¨äºæ¼”ç¤ºçš„ç¤ºä¾‹æ–‡ä»¶"""
    temp_dir = tempfile.mkdtemp()
    
    # åˆ›å»ºç¤ºä¾‹æ–‡æœ¬æ–‡ä»¶
    sample_text = """# Python ç¼–ç¨‹å…¥é—¨æŒ‡å—

## ç¬¬ä¸€ç« ï¼šPython ç®€ä»‹

Python æ˜¯ä¸€ç§å¹¿æ³›ä½¿ç”¨çš„é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œç”± Guido van Rossum äº 1989 å¹´åˆ›å»ºã€‚
Python çš„è®¾è®¡å“²å­¦å¼ºè°ƒä»£ç çš„å¯è¯»æ€§å’Œç®€æ´æ€§ã€‚

### 1.1 Python çš„ç‰¹ç‚¹

- **ç®€å•æ˜“å­¦**ï¼šPython è¯­æ³•ç®€æ´æ¸…æ™°
- **è·¨å¹³å°**ï¼šå¯åœ¨ Windowsã€Macã€Linux ä¸Šè¿è¡Œ
- **ä¸°å¯Œçš„åº“**ï¼šæ‹¥æœ‰å¤§é‡ç¬¬ä¸‰æ–¹åº“æ”¯æŒ

### 1.2 å®‰è£… Python

å¯ä»¥ä» python.org ä¸‹è½½æœ€æ–°ç‰ˆæœ¬ã€‚

## ç¬¬äºŒç« ï¼šåŸºç¡€è¯­æ³•

### 2.1 å˜é‡å’Œæ•°æ®ç±»å‹

Python æ”¯æŒå¤šç§æ•°æ®ç±»å‹ï¼š
- æ•´æ•° (int)
- æµ®ç‚¹æ•° (float)
- å­—ç¬¦ä¸² (str)
- åˆ—è¡¨ (list)
- å­—å…¸ (dict)

### 2.2 æ§åˆ¶æµç¨‹

Python ä½¿ç”¨ç¼©è¿›æ¥è¡¨ç¤ºä»£ç å—ï¼š

```python
if condition:
    # æ‰§è¡Œä»£ç 
    pass
elif other_condition:
    # å…¶ä»–ä»£ç 
    pass
else:
    # é»˜è®¤ä»£ç 
    pass
```

## ç¬¬ä¸‰ç« ï¼šå‡½æ•°

å‡½æ•°æ˜¯ç»„ç»‡ä»£ç çš„é‡è¦æ–¹å¼ï¼š

```python
def greet(name):
    return f"Hello, {name}!"
```

## æ€»ç»“

Python æ˜¯ä¸€é—¨ä¼˜ç§€çš„ç¼–ç¨‹è¯­è¨€ï¼Œé€‚åˆåˆå­¦è€…å…¥é—¨ï¼Œä¹Ÿèƒ½æ»¡è¶³ä¸“ä¸šå¼€å‘éœ€æ±‚ã€‚
"""
    
    txt_path = os.path.join(temp_dir, "python_guide.txt")
    with open(txt_path, "w", encoding="utf-8") as f:
        f.write(sample_text)
    
    # åˆ›å»º CSV ç¤ºä¾‹
    csv_content = """å§“å,å¹´é¾„,åŸå¸‚,èŒä¸š
å¼ ä¸‰,28,åŒ—äº¬,å·¥ç¨‹å¸ˆ
æå››,32,ä¸Šæµ·,äº§å“ç»ç†
ç‹äº”,25,å¹¿å·,è®¾è®¡å¸ˆ
èµµå…­,35,æ·±åœ³,æ•°æ®åˆ†æå¸ˆ
"""
    csv_path = os.path.join(temp_dir, "employees.csv")
    with open(csv_path, "w", encoding="utf-8") as f:
        f.write(csv_content)
    
    # åˆ›å»º JSON ç¤ºä¾‹
    json_content = """{
    "company": "ç§‘æŠ€æœ‰é™å…¬å¸",
    "founded": 2020,
    "products": [
        {"name": "äº§å“A", "price": 99.9, "category": "è½¯ä»¶"},
        {"name": "äº§å“B", "price": 199.9, "category": "æœåŠ¡"},
        {"name": "äº§å“C", "price": 299.9, "category": "ç¡¬ä»¶"}
    ],
    "locations": ["åŒ—äº¬", "ä¸Šæµ·", "æ·±åœ³"]
}"""
    json_path = os.path.join(temp_dir, "company.json")
    with open(json_path, "w", encoding="utf-8") as f:
        f.write(json_content)
    
    return temp_dir, txt_path, csv_path, json_path

# ============================================================
# ç¤ºä¾‹ 1ï¼šåŸºæœ¬æ–‡æœ¬æ–‡ä»¶åŠ è½½
# ============================================================

def basic_text_loading(txt_path: str):
    """
    åŠ è½½å’Œå¤„ç†æ–‡æœ¬æ–‡ä»¶
    """
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹ 1ï¼šåŸºæœ¬æ–‡æœ¬æ–‡ä»¶åŠ è½½")
    print("=" * 60)

    # è¯»å–æ–‡ä»¶
    with open(txt_path, "r", encoding="utf-8") as f:
        content = f.read()
    
    # åˆ›å»º Document å¯¹è±¡
    doc = Document(
        page_content=content,
        metadata={
            "source": txt_path,
            "type": "text",
            "encoding": "utf-8"
        }
    )
    
    print(f"ğŸ“„ å·²åŠ è½½æ–‡ä»¶: {os.path.basename(txt_path)}")
    print(f"   å­—ç¬¦æ•°: {len(doc.page_content)}")
    print(f"   å…ƒæ•°æ®: {doc.metadata}")
    
    # ä½¿ç”¨ LLM åˆ†ææ–‡æ¡£
    messages = [
        SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªæ–‡æ¡£åˆ†æä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹æ–‡æ¡£çš„ç»“æ„å’Œä¸»è¦å†…å®¹ã€‚ç”¨ä¸­æ–‡ç®€æ´å›ç­”ã€‚"),
        HumanMessage(content=f"æ–‡æ¡£å†…å®¹ï¼š\n\n{doc.page_content[:2000]}")  # é™åˆ¶é•¿åº¦
    ]
    
    response = model.invoke(messages)
    print("\nğŸ“Š æ–‡æ¡£åˆ†æï¼š")
    print(response.content)
    
    return doc

# ============================================================
# ç¤ºä¾‹ 2ï¼šæ–‡æ¡£åˆ†å—
# ============================================================

def document_chunking(txt_path: str):
    """
    å°†é•¿æ–‡æ¡£åˆ†å‰²æˆå°å—
    """
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹ 2ï¼šæ–‡æ¡£åˆ†å—")
    print("=" * 60)

    # è¯»å–æ–‡ä»¶
    with open(txt_path, "r", encoding="utf-8") as f:
        content = f.read()
    
    doc = Document(page_content=content, metadata={"source": txt_path})
    
    # åˆ›å»ºæ–‡æœ¬åˆ†å‰²å™¨
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,        # æ¯å—çº¦ 500 å­—ç¬¦
        chunk_overlap=50,      # é‡å  50 å­—ç¬¦ä»¥ä¿æŒä¸Šä¸‹æ–‡
        separators=["\n## ", "\n### ", "\n\n", "\n", "ã€‚", " "],
        length_function=len
    )
    
    # åˆ†å‰²æ–‡æ¡£
    chunks = splitter.split_documents([doc])
    
    print(f"ğŸ“‘ åŸæ–‡æ¡£é•¿åº¦: {len(content)} å­—ç¬¦")
    print(f"ğŸ“‘ åˆ†å‰²æˆ {len(chunks)} ä¸ªå—")
    print("\nå„å—ä¿¡æ¯ï¼š")
    
    for i, chunk in enumerate(chunks[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª
        print(f"  å— {i+1}: {len(chunk.page_content)} å­—ç¬¦")
        print(f"    å¼€å¤´: {chunk.page_content[:50]}...")
    
    if len(chunks) > 5:
        print(f"  ... è¿˜æœ‰ {len(chunks) - 5} ä¸ªå—")
    
    return chunks

# ============================================================
# ç¤ºä¾‹ 3ï¼šCSV æ–‡ä»¶å¤„ç†
# ============================================================

def csv_processing(csv_path: str):
    """
    å¤„ç† CSV æ–‡ä»¶
    """
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹ 3ï¼šCSV æ–‡ä»¶å¤„ç†")
    print("=" * 60)

    import csv
    
    # è¯»å– CSV
    with open(csv_path, "r", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        rows = list(reader)
    
    # å°†æ¯è¡Œè½¬ä¸º Document
    documents = []
    for i, row in enumerate(rows):
        doc = Document(
            page_content=str(row),
            metadata={"source": csv_path, "row": i + 1}
        )
        documents.append(doc)
    
    print(f"ğŸ“Š å·²åŠ è½½ {len(documents)} æ¡è®°å½•")
    print("\nå‰å‡ æ¡è®°å½•ï¼š")
    for doc in documents[:3]:
        print(f"  ç¬¬ {doc.metadata['row']} è¡Œ: {doc.page_content}")
    
    # ä½¿ç”¨ LLM åˆ†æ CSV æ•°æ®
    csv_content = "\n".join([doc.page_content for doc in documents])
    
    messages = [
        SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªæ•°æ®åˆ†æä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹æ•°æ®å¹¶ç»™å‡ºè§è§£ã€‚ç”¨ä¸­æ–‡å›ç­”ã€‚"),
        HumanMessage(content=f"æ•°æ®å†…å®¹ï¼š\n{csv_content}")
    ]
    
    response = model.invoke(messages)
    print("\nğŸ“ˆ æ•°æ®åˆ†æï¼š")
    print(response.content)
    
    return documents

# ============================================================
# ç¤ºä¾‹ 4ï¼šJSON æ–‡ä»¶å¤„ç†
# ============================================================

def json_processing(json_path: str):
    """
    å¤„ç† JSON æ–‡ä»¶
    """
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹ 4ï¼šJSON æ–‡ä»¶å¤„ç†")
    print("=" * 60)

    import json
    
    # è¯»å– JSON
    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    # å°† JSON è½¬ä¸ºæ ¼å¼åŒ–æ–‡æœ¬
    formatted_json = json.dumps(data, ensure_ascii=False, indent=2)
    
    doc = Document(
        page_content=formatted_json,
        metadata={
            "source": json_path,
            "type": "json",
            "keys": list(data.keys())
        }
    )
    
    print("ğŸ“‹ JSON ç»“æ„ï¼š")
    print(f"   é¡¶çº§é”®: {doc.metadata['keys']}")
    print("\nå†…å®¹é¢„è§ˆï¼š")
    print(formatted_json[:500])
    
    # ä½¿ç”¨ LLM ç†è§£ JSON ç»“æ„
    messages = [
        SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªæ•°æ®ç»“æ„ä¸“å®¶ã€‚è¯·è§£é‡Šè¿™ä¸ª JSON çš„ç»“æ„å’Œç”¨é€”ã€‚ç”¨ä¸­æ–‡å›ç­”ã€‚"),
        HumanMessage(content=f"JSON å†…å®¹ï¼š\n{formatted_json}")
    ]
    
    response = model.invoke(messages)
    print("\nğŸ” ç»“æ„åˆ†æï¼š")
    print(response.content)
    
    return doc

# ============================================================
# ç¤ºä¾‹ 5ï¼šæ–‡æ¡£é—®ç­”
# ============================================================

def document_qa(txt_path: str):
    """
    åŸºäºæ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜
    """
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹ 5ï¼šæ–‡æ¡£é—®ç­”")
    print("=" * 60)

    # è¯»å–æ–‡ä»¶
    with open(txt_path, "r", encoding="utf-8") as f:
        content = f.read()
    
    questions = [
        "Python æ˜¯ä»€ä¹ˆæ—¶å€™åˆ›å»ºçš„ï¼Ÿ",
        "Python æœ‰å“ªäº›ä¸»è¦æ•°æ®ç±»å‹ï¼Ÿ",
        "å¦‚ä½•åœ¨ Python ä¸­å®šä¹‰å‡½æ•°ï¼Ÿ"
    ]
    
    print("ğŸ“– åŸºäºæ–‡æ¡£å›ç­”é—®é¢˜ï¼š\n")
    
    for question in questions:
        messages = [
            SystemMessage(content=f"""ä½ æ˜¯ä¸€ä¸ªæ–‡æ¡£é—®ç­”åŠ©æ‰‹ã€‚æ ¹æ®ä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ã€‚
å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´æ˜"æ–‡æ¡£ä¸­æœªæåŠæ­¤ä¿¡æ¯"ã€‚
ç”¨ä¸­æ–‡ç®€æ´å›ç­”ã€‚

æ–‡æ¡£å†…å®¹ï¼š
{content}"""),
            HumanMessage(content=question)
        ]
        
        response = model.invoke(messages)
        
        print(f"â“ é—®é¢˜: {question}")
        print(f"ğŸ’¬ å›ç­”: {response.content}\n")

# ============================================================
# ç¤ºä¾‹ 6ï¼šå¤šæ–‡ä»¶åˆå¹¶åˆ†æ
# ============================================================

def multi_file_analysis(temp_dir: str, txt_path: str, csv_path: str, json_path: str):
    """
    åˆå¹¶å¤šä¸ªæ–‡ä»¶è¿›è¡Œç»¼åˆåˆ†æ
    """
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹ 6ï¼šå¤šæ–‡ä»¶åˆå¹¶åˆ†æ")
    print("=" * 60)

    # åŠ è½½æ‰€æœ‰æ–‡ä»¶
    documents = []
    
    # æ–‡æœ¬æ–‡ä»¶
    with open(txt_path, "r", encoding="utf-8") as f:
        documents.append(Document(
            page_content=f.read()[:1000],  # é™åˆ¶é•¿åº¦
            metadata={"source": "python_guide.txt", "type": "tutorial"}
        ))
    
    # CSV æ–‡ä»¶
    with open(csv_path, "r", encoding="utf-8") as f:
        documents.append(Document(
            page_content=f.read(),
            metadata={"source": "employees.csv", "type": "data"}
        ))
    
    # JSON æ–‡ä»¶
    with open(json_path, "r", encoding="utf-8") as f:
        documents.append(Document(
            page_content=f.read(),
            metadata={"source": "company.json", "type": "config"}
        ))
    
    print(f"ğŸ“ å·²åŠ è½½ {len(documents)} ä¸ªæ–‡ä»¶ï¼š")
    for doc in documents:
        print(f"   - {doc.metadata['source']} ({doc.metadata['type']})")
    
    # åˆå¹¶å†…å®¹
    combined_content = "\n\n---\n\n".join([
        f"ã€{doc.metadata['source']}ã€‘\n{doc.page_content}"
        for doc in documents
    ])
    
    # ç»¼åˆåˆ†æ
    messages = [
        SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªç»¼åˆåˆ†æä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹å¤šä¸ªæ–‡ä»¶çš„å†…å®¹ï¼Œæ‰¾å‡ºå®ƒä»¬ä¹‹é—´çš„è”ç³»ï¼Œå¹¶ç»™å‡ºç»¼åˆè§è§£ã€‚ç”¨ä¸­æ–‡å›ç­”ã€‚"),
        HumanMessage(content=combined_content)
    ]
    
    response = model.invoke(messages)
    
    print("\nğŸ”— ç»¼åˆåˆ†æï¼š")
    print(response.content)
    
    return documents

# ============================================================
# ä¸»ç¨‹åº
# ============================================================

if __name__ == "__main__":
    print("æ–‡ä»¶å¤„ç†æ•™ç¨‹")
    print("=" * 60)
    
    # åˆ›å»ºç¤ºä¾‹æ–‡ä»¶
    temp_dir, txt_path, csv_path, json_path = create_sample_files()
    print(f"å·²åˆ›å»ºç¤ºä¾‹æ–‡ä»¶äº: {temp_dir}")
    
    try:
        # è¿è¡Œç¤ºä¾‹
        basic_text_loading(txt_path)
        document_chunking(txt_path)
        csv_processing(csv_path)
        json_processing(json_path)
        document_qa(txt_path)
        multi_file_analysis(temp_dir, txt_path, csv_path, json_path)
        
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        import shutil
        shutil.rmtree(temp_dir)
        print("\nå·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶")
    
    print("\n" + "=" * 60)
    print("âœ… æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆï¼")
    print("=" * 60)
